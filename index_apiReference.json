[
    {
        "title": "API Reference",
        "id": "introduction",
        "mainUrl": "/docs/api-reference/introduction",
        "sections": [
            {
                "title": "API Interaction",
                "id": "api-interaction",
                "contents": [
                    "You can interact with the API through HTTP or Websocket requests from any language, via our official Python bindings or our official Node.js libraries.",
                    "To install the official Python bindings, run the following command:",
                    "To install the official Node.js library, run the following command in your Node.js project directory:"
                ]
            },
            {
                "title": "Python SDK",
                "id": "python-sdk-install",
                "contents": [
                    "To install the official Python bindings, run the following command:"
                ]
            },
            {
                "title": "Node.js SDK",
                "id": "nodejs-sdk-install",
                "contents": [
                    "To install the official Node.js library, run the following command in your Node.js project directory:"
                ]
            }
        ]
    },
    {
        "title": "Authentication",
        "id": "authentication",
        "mainUrl": "/docs/api-reference/authentication",
        "sections": [
            {
                "title": "API Keys",
                "id": "api-keys",
                "contents": [
                    "API Keys",
                    "Our API uses API key authentication to secure access to all endpoints. You must include a valid API key in every request to authenticate your identity and ensure authorized access."
                ]
            },
            {
                "title": "Obtain Key",
                "id": "obtain-api-key",
                "contents": [
                    "You can obtain an API key byregistering for an accountand navigating to the API Keys section in your dashboard."
                ]
            },
            {
                "title": "Authenticated Requests",
                "id": "authenticated-requests",
                "contents": [
                    "Using the API Key",
                    "Include your API key in the X-API-Key header as follows:"
                ]
            }
        ]
    },
    {
        "title": "API Authentication ",
        "id": "authentication",
        "mainUrl": "/docs/api-reference/authentication",
        "sections": [
            {
                "title": "API Authentication Using Python",
                "id": "auth-pythonSDK",
                "contents": [
                    "Using the Alle-AI Python Package",
                    "Example of initializing the Alle-AI client in Python:"
                ]
            },
            {
                "title": "API Authentication Using Node.js",
                "id": "auth-nodejs",
                "contents": [
                    "Using the Alle-AI Node.js Package",
                    "Example of initializing the Alle-AI client in Node.js:"
                ]
            }
        ]
    },
    {
        "title": "`",
        "id": "sdk",
        "mainUrl": "/docs/api-reference/sdk",
        "sections": [
            {
                "title": "Python SDK ",
                "id": "python-sdk-api",
                "contents": [
                    "Python SDK",
                    "The official Alleai Python SDK provides a simple and intuitive interface for interacting with the API. Here\u2019s how to get started:"
                ]
            },
            {
                "title": "Node.js SDK",
                "id": "nodejs-sdk-api",
                "contents": [
                    "Node.js SDK",
                    "The official Alleai Node.js SDK is built for modern JavaScript and TypeScript applications. Here\u2019s how to use it:"
                ]
            },
            {
                "title": "Best Practices for Using SDKs",
                "id": "best-practise-sdk",
                "contents": [
                    "Best Practices for Using SDKs",
                    "To get the most out of the Alleai SDKs, follow these best practices:",
                    "Environment Variables:Store your API key in environment variables to keep it secure and avoid hardcoding it in your application.",
                    "Error Handling:Always wrap API calls in try-catch blocks to handle errors gracefully.",
                    "TypeScript Support:If you're using Node.js, consider using TypeScript for better type safety and autocompletion.",
                    "Keep SDKs Updated:Regularly update the SDKs to benefit from the latest features and bug fixes."
                ]
            }
        ]
    },
    {
        "title": "Chat-Endpoints",
        "id": "chat-endpoints",
        "mainUrl": "/docs/api-reference/chat-endpoints",
        "sections": [
            {
                "title": "Chat Completion",
                "id": "chat-chat-completion",
                "contents": [
                    "The Chat Completion endpoint is our unified gateway to multiple AI models including ChatGPT, Claude, Gemini, and more. This endpoint allows you to generate human-like text responses using state-of-the-art language models.",
                    "Overview",
                    "Access multiple AI models through a single endpoint. Choose from:",
                    "OpenAI's ChatGPT (GPT-3.5, GPT-4)",
                    "Anthropic's Claude",
                    "Google's Gemini",
                    "And more leading AI models"
                ]
            },
            {
                "title": "Completion Endpoints",
                "id": "chat-completion-ends",
                "contents": [
                    "All completions API requests should be made to this URL using HTTPS."
                ]
            },
            {
                "title": "Completion Endpoints",
                "id": "chat-completion-ends",
                "contents": [
                    "All completions API requests should be made to this URL using HTTPS."
                ]
            },
            {
                "title": "Request Parameters",
                "id": "completion-params",
                "contents": [
                    "A list of AImodel namesthat will process your request. You can include options like 'gpt-4', 'deepseek-1', or 'claude-3.5-sonnet' to decide which models generate responses or handle tasks, based on what each one is good at.",
                    "The full set of inputs and context for the models to work with. This includes optionalsystem instructionsto guide the models' behavior (like setting their role or tone),user inputssuch astextquestions, audio recordings, images, or videos for the models to respond to, and priorassistant responsesorganized by model name to keep the conversation flowing. Each message can contain content in formats liketext,audio URLs,image URLs, orvideo URLs, giving you flexibility in how you communicate with the models.",
                    "Controls how the models deliver their output to you. You specify themain format\u2014whether you wanttext, anaudio URL, animage URL, or avideo URL\u2014and can optionally set different formats forspecific modelsif you're using more than one. This ensures the response matches what you need, tailored to each model's contribution.",
                    "Lets you decide if the models can search the web to include up-to-date or outside information in their responses. Turn it on withtrueto broaden their knowledge, or keep it off withfalseto rely only on what the models already know.",
                    "Configures how to get ashorter versionof the response, perfect for quick insights. You can choose theformat\u2014liketext,audio URL,image URL, orvideo URL\u2014and pick whichmodels(from your 'models' list) should create the summary, letting you customize how it's condensed and presented.",
                    "Sets up how tomerge responsesfrom multiplemodelsinto one cohesive output. You define theformat\u2014text,audio URL,image URL, orvideo URL\u2014and select whichmodels' answers to blend, combining their strengths into a single result that suits your needs.",
                    "Adjusts howcreativeorpredictablethe models' responses are. Use alower value(like 0.1) for straightforward, focused answers, or ahigher value(up to 1.0) for more imaginative or varied replies, depending on the tone you're aiming for.",
                    "Caps thelengthof the response, measured insmall unitslike words or characters. Set a number to keep answersbriefor allow them to runlonger, giving you control over how much detail you get back.",
                    "Shapes howvariedthe models' responses can be. Alower value(like 0.5) keeps answers focused on the most likely ideas, while ahigher value(up to 1.0) lets the models explore a wider range of possibilities, balancingfocuswithdiversity.",
                    "Controls how much the modelsrepeatthemselves. Ahigher value(up to 2.0) pushes them to avoid reusing phrases or ideas, keeping thingsfresh, while a lower or negative value (down to -2.0) lets themrepeatmore if that's what you want.",
                    "Influences whether the models stick to what's already been said or bring upnew topics. Ahigher value(up to 2.0) encourages fresh ideas, while a lower or negative value (down to -2.0) keeps them focused on thecurrent discussion.",
                    "Determines if you get the responsebit by bitas it's being created (withtrue) orall at oncewhen it's finished (withfalse). Streaming is handy forreal-timeupdates, while waiting gives you the complete answer in one go.",
                    "fine-tunessettings for individual models, overriding the general options. You can adjust things likesystem instructions,creativity level,response length, orrepetition controlsforspecific models(like 'gpt-4' or 'deepseek-1'), giving you precise control over each one's behavior."
                ]
            },
            {
                "title": "Request Headers",
                "id": "request-headers-chat",
                "contents": [
                    "Request Headers"
                ]
            },
            {
                "title": "API Response Format",
                "id": "response-format-chat",
                "contents": [
                    "Indicates if the API request was successful.",
                    "Container for the main response data.",
                    "Unique identifier for the response.",
                    "Type of object returned (e.g., 'ai.completions').",
                    "Information about when the response was created.",
                    "Date and time when the response was created.",
                    "Timezone for the creation timestamp.",
                    "List of models used in the API call.",
                    "Response content from each model, organized by model name.",
                    "Response data for a specific model.",
                    "Name of the model (e.g., 'gpt-4o', 'gemini-2-5-pro').",
                    "The response content from the model.",
                    "Reason for stopping the response (e.g., 'stop', 'STOP').",
                    "Token usage details for the response.",
                    "Model-specific API usage details.",
                    "Total number of requests made.",
                    "Total cost for model usage.",
                    "Overall API usage and cost details.",
                    "Cost associated with input tokens.",
                    "Cost associated with output tokens.",
                    "Total cost of the API usage."
                ]
            }
        ]
    },
    {
        "title": "Web-Search",
        "id": "chat-search",
        "mainUrl": "/docs/api-reference/chat-search",
        "sections": [
            {
                "title": "Using Web-search in chat completions",
                "id": "web-search-completions",
                "contents": [
                    "Usingweb_searchin Completion Endpoint"
                ]
            },
            {
                "title": "Web-search response body",
                "id": "web-search-response",
                "contents": [
                    "Web Search Results",
                    "If thewebsearchparameter was set totruein the API request, the response will include aweb_search_resultsfield. This field contains the results of web searches performed by the model to enhance its response."
                ]
            },
            {
                "title": "Web-search response body",
                "id": "web-search-response",
                "contents": [
                    "Web Search Results",
                    "If thewebsearchparameter was set totruein the API request, the response will include aweb_search_resultsfield. This field contains the results of web searches performed by the model to enhance its response."
                ]
            },
            {
                "title": "Web Search Endpoints",
                "id": "web-search-endpoint",
                "contents": [
                    "Dedicated Web Search Endpoints"
                ]
            }
        ]
    },
    {
        "title": "Model Output Comparison",
        "id": "chat-comparison",
        "mainUrl": "/docs/api-reference/chat-comparison",
        "sections": [
            {
                "title": "Compare model responses",
                "id": "compare-models",
                "contents": [
                    "Model OutputComparison"
                ]
            },
            {
                "title": "Model Output Comparison",
                "id": "model-output-compare",
                "contents": [
                    "Thecomparisonfield is anoptionalparameter in the API request. Including this field enables the API to perform comparisons across different AI models and return the comparison results along with the regular model outputs. If thecomparisonfield is omitted, no model comparisons will be performed, and only the standard model outputs will be returned.",
                    "If you include thecomparisonfield, it accepts an array of comparison objects, where each object defines a specific comparison type and the models to be included.",
                    "Comparison Types",
                    "Thetypefield specifies the type of comparison to be performed. The following types are currently supported:",
                    "text: Compares the text output of the specified models.",
                    "audio_url: Compares the audio content accessible at the provided URLs.",
                    "Models - Format(Important)",
                    "Themodelsfield is an array of strings. Each string specifies acombinationof models to be used in the comparison. Models within each string are joined together with plus signs (+). This indicates that the models will be used in a combined or ensemble manner for that specific comparison. For example:",
                    "\"gpt-4o+deepseek-r1+claude-3.5-sonnet\": This specifies a comparison involving GPT-4, Deepseek, and Claude.",
                    "\"selected_model_one+yet-another-model\": Another example with audio models.",
                    "Each string in themodelsarray represents a distinct set of models to be usedtogetherin a single comparison. You can have multiple strings in themodelsarray if you want to compare different combinations of models for the same comparisontype."
                ]
            }
        ]
    },
    {
        "title": "Summarize response from multiple chat models",
        "id": "chat-summary",
        "mainUrl": "/docs/api-reference/chat-summary",
        "sections": [
            {
                "title": "Summary in Completion Endpoint",
                "id": "summary-completion",
                "contents": [
                    "Summaryin Completion Endpoint"
                ]
            },
            {
                "title": " Dedicated Summary Endpoints",
                "id": "summary-endpoints",
                "contents": [
                    "Dedicated Summary Endpoints",
                    "For a streamlinedsummarywithout individual model outputs, use the dedicatedsummaryendpoints at [baseUrl]/summary. These endpoints mirror the completion request structure usingmessagesfor your query,response_formatto define the output (text, audio URL, etc.), and settings liketemperaturefor fine-tuning but focus solely on deliveringsummaryresults from the models. Unlike the completion endpoint, you won't get per-model responses here; it's just the distilledsummary, ideal for scenarios where you want a lightweight, essential take without the extra detail."
                ]
            }
        ]
    },
    {
        "title": "Combine responses from chat models",
        "id": "chat-combination",
        "mainUrl": "/docs/api-reference/chat-combination",
        "sections": [
            {
                "title": "Combinations in Completion Endpoint",
                "id": "combination-completion",
                "contents": [
                    "Combinationsin Completion Endpoint"
                ]
            },
            {
                "title": " Dedicated Combination Endpoints",
                "id": "combination-endpoints",
                "contents": [
                    "Dedicated Combination Endpoints",
                    "For a focused result that skips individual model outputs, use the dedicatedcombinationendpoint at [baseUrl]/combinations. This endpoint mirrors the completion request body,messagesfor your query,response_formatfor output style, andtemperaturefor adjustments, but this only delivers the combined results from the models you specify. Using the samecombinationarray (e.g., \"combination\": [{\"type\": \"text\", \"models\": [\"gpt-4o+deepseek-r1+claude-3.5-sonnet\"]}, {\"type\": \"audio_url\", \"models\": [\"gpt-4o+claude-3.5-sonnet\"]}), you pick which models to blend with the + notation, and that's all you get back no separate responses. It's a clean, efficient way to get a synthesized output when you don't need the raw model-by-model breakdown."
                ]
            }
        ]
    },
    {
        "title": "Image Generation",
        "id": "image-generation",
        "mainUrl": "/docs/api-reference/image-generation",
        "sections": [
            {
                "title": "Image Generation API",
                "id": "image-generation-api",
                "contents": [
                    "Image Generation API",
                    "Multi-Model Image Generation API",
                    "Our Multi-Model Image Generation API allows developers to leverage multiple AI models simultaneously for image generation and editing tasks. This unique approach enables you to compare outputs across different models, provide users with diverse creative options, and select the best result for your specific use case.",
                    "The API supports two primary operations:",
                    "Text-to-Image Generation :Create images from text prompts across multiple models.",
                    "Image Editing :Modify existing images using text instructions across multiple models.",
                    "This documentation provides detailed information about endpoints, request formats, response structures, and code examples to help you integrate these capabilities into your applications.",
                    "Authentication",
                    "All API requests require authentication using an API key.",
                    "You can obtain an API key byregistering for an accountand navigating to the API Keys section in your dashboard.",
                    "Authentication",
                    "All API requests require authentication using an API key.",
                    "You can obtain an API key byregistering for an accountand navigating to the API Keys section in your dashboard."
                ]
            }
        ]
    },
    {
        "title": "Text-to-Image API",
        "id": "image-generation",
        "mainUrl": "/docs/api-reference/image-generation",
        "sections": [
            {
                "title": "Generate Images from Text",
                "id": "text-to-image-api",
                "contents": [
                    "Text-to-Image API",
                    "The Text-to-Image API transforms text descriptions into visual imagery using multiple AI models simultaneously. This parallel approach allows you to:",
                    "Compare stylistic differences between models",
                    "Offer diverse creative options to your users",
                    "Experiment with prompt engineering across different model architectures",
                    "Select the most suitable output for your specific needs",
                    "Request Body",
                    "An array of selected image models for the API call.",
                    "A text description of the desired image(s).",
                    "The number of images to generate. Optional.",
                    "The height of the generated image in pixels. Optional.",
                    "The width of the generated image in pixels. Optional.",
                    "A seed value for reproducible image generation. Optional.",
                    "The style of the generated image. Optional."
                ]
            }
        ]
    },
    {
        "title": "Edit Images via API",
        "id": "image-generation-edits",
        "mainUrl": "/docs/api-reference/image-generation-edits",
        "sections": [
            {
                "title": "Create image edits",
                "id": "image-edits-api",
                "contents": [
                    "Create image edits",
                    "Creates, edits or extended image given an original image and a prompt.",
                    "Request body",
                    "An array of models to use for image editing.",
                    "base64-encoded string or a web URL pointing to the image to edit.",
                    "A text description of the desired edit. Maximum length is 1000 characters."
                ]
            }
        ]
    },
    {
        "title": "Audio Generation API",
        "id": "audio-text-to-speech",
        "mainUrl": "/docs/api-reference/audio-text-to-speech",
        "sections": [
            {
                "title": "Audio Generation API",
                "id": "audio-generation-api",
                "contents": [
                    "Audio Generation API",
                    "Learn how to turn audio into text or text into audio by combining multiple audio models.",
                    "The API supports three primary operations:",
                    "Create speech :Generates audio from the input text.",
                    "Create transcription :Transcribes audio into the input language.",
                    "Create audio :Generate sounds, music, and other audio formats from text by combining power of multiple audio models",
                    "Authentication",
                    "All API requests require authentication using an API key.",
                    "You can obtain an API key byregistering for an accountand navigating to the API Keys section in your dashboard.",
                    "Authentication",
                    "All API requests require authentication using an API key.",
                    "You can obtain an API key byregistering for an accountand navigating to the API Keys section in your dashboard."
                ]
            }
        ]
    },
    {
        "title": "Text-to-Speech",
        "id": "audio-text-to-speech",
        "mainUrl": "/docs/api-reference/audio-text-to-speech",
        "sections": [
            {
                "title": "Text-to-Speech API",
                "id": "text-to-speech-api",
                "contents": [
                    "Generates audio from the input text with multiple audio models.",
                    "Request Body",
                    "Array of selected audio models for the text-to-speech API call.",
                    "The text to generate audio for.",
                    "The voice to use for audio generation. Optional.",
                    "Model-specific parameters for audio generation."
                ]
            }
        ]
    },
    {
        "title": "Speech-to-Text",
        "id": "audio-speech-to-text",
        "mainUrl": "/docs/api-reference/audio-speech-to-text",
        "sections": [
            {
                "title": "Speech-to-Text API",
                "id": "speech-to-text-api",
                "contents": [
                    "Create transcription",
                    "Transcribes audio into the input language.",
                    "Request body",
                    "Array of selected models for audio transcription.",
                    "Path or a web URL pointing to the audio file to transcribe. Supported formats include flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.",
                    "Model-specific parameters for audio transcription."
                ]
            }
        ]
    },
    {
        "title": "Create audio",
        "id": "audio-generate",
        "mainUrl": "/docs/api-reference/audio-generate",
        "sections": [
            {
                "title": "Audio Generation API",
                "id": "audio-generate-api",
                "contents": [
                    "Create audio",
                    "Generate all kinds of sound from supported AI modes",
                    "Request body",
                    "Array of selected audio models for generating audio from text.",
                    "The text description to generate audio from. Maximum length is 4096 characters.",
                    "The duration of the generated audio in seconds. Optional.",
                    "The tempo of the generated audio in beats per minute (BPM). Optional.",
                    "The musical genre for the generated audio (e.g., 'jazz', 'rock'). Optional.",
                    "Array of instruments to include in the audio (e.g., 'piano', 'drums'). Optional.",
                    "The quality of the generated audio: 'standard' or 'hd'. Optional, defaults to 'standard'.",
                    "The audio output format. Supported values are 'mp3', 'wav', or 'ogg'. Optional.",
                    "Whether the generated audio should be loopable. Optional.",
                    "The mood of the generated audio (e.g., 'happy', 'calm'). Optional.",
                    "The volume level of the generated audio. Optional."
                ]
            }
        ]
    },
    {
        "title": "Text-to-Video",
        "id": "video-generation",
        "mainUrl": "/docs/api-reference/video-generation",
        "sections": [
            {
                "title": "Text-to-video API",
                "id": "text-to-video-api",
                "contents": [
                    "Text-to-Video API",
                    "Generates videos from the input text with multiple video models.",
                    "Request Body",
                    "Array of selected models for generating video from text.",
                    "The text description to generate video from. Maximum length is 4096 characters.",
                    "The duration of the generated video in seconds. Optional.",
                    "Whether the video should loop playback. Optional.",
                    "The aspect ratio of the video (e.g., '16:9', '4:3'). Optional.",
                    "The frame rate of the video in frames per second. Optional.",
                    "The pixel dimensions of the video (e.g., '1280x720'). Optional.",
                    "The resolution quality of the video (e.g., '720p'). Optional.",
                    "A seed value for reproducible video generation. Optional.",
                    "Additional parameters specific to the selected models. Optional.",
                    "Asynchronous Processing & Job Status",
                    "Video generation is an asynchronous process and may take some time to complete depending on the complexity of your request. When you submit a video generation request, the API responds immediately with ajob_Id(orrequest_Id), indicating your request has been queued for processing.",
                    "You can use thisrequest_Idto poll the status of your video generation by making a GET request to the status endpoint:",
                    "The status endpoint will return the current state of your job, such asInProgressorCompleted. Once the job is completed, the response will include the generated video URL and related metadata.",
                    "This approach allows you to efficiently manage long-running video generation tasks and retrieve results once processing is finished."
                ]
            }
        ]
    }
]